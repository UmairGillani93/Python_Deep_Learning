{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index:  {'in': 1, 'the': 2, 'town': 3, 'of': 4, 'athy': 5, 'one': 6, 'jeremy': 7, 'lanigan': 8, 'battered': 9, 'away': 10, 'till': 11, 'he': 12, 'hadnt': 13, 'had': 14, 'a': 15, 'pond': 16, 'his': 17, 'father': 18, 'died': 19, 'and': 20, 'made': 21, 'him': 22, 'rich': 23}\n",
      "\n",
      "Total unique words:  24\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() \n",
    "\n",
    "data = 'In the town of Athy one Jeremy lanigan \\nBattered away till he hadnt had a pond. \\nHis father died and made him rich'\n",
    "\n",
    "corpus = data.lower().split('\\n')\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(\"Word Index: \", word_index)\n",
    "print(\"\\nTotal unique words: \", total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in the town of athy one jeremy lanigan ',\n",
       " 'battered away till he hadnt had a pond. ',\n",
       " 'his father died and made him rich']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [1, 2, 3], [1, 2, 3, 4], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7, 8], [9, 10], [9, 10, 11], [9, 10, 11, 12], [9, 10, 11, 12, 13], [9, 10, 11, 12, 13, 14], [9, 10, 11, 12, 13, 14, 15], [9, 10, 11, 12, 13, 14, 15, 16], [17, 18], [17, 18, 19], [17, 18, 19, 20], [17, 18, 19, 20, 21], [17, 18, 19, 20, 21, 22], [17, 18, 19, 20, 21, 22, 23]]\n",
      "\n",
      "Max sequence length:  20\n"
     ]
    }
   ],
   "source": [
    "input_sequence = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0] # creates to word index for each word in a sentence\n",
    "    for i in range(1, len(token_list)): \n",
    "        n_gram_sequence = token_list[:i+1] # creates n_grams sequence \n",
    "        input_sequence.append(n_gram_sequence)\n",
    "        \n",
    "print(input_sequence)\n",
    "\n",
    "# pad the sequences \n",
    "max_sequence_len = len([max(x) for x in input_sequence])\n",
    "input_sequences = np.array(pad_sequences(input_sequence, maxlen = max_sequence_len, padding = 'pre'))\n",
    "\n",
    "# print(max_sequence_len)\n",
    "print('\\nMax sequence length: ', max_sequence_len)\n",
    "\n",
    "# create predictors and labels\n",
    "xs, labels = input_sequences[:-1], input_sequences[-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes = total_words) # categorizing the labels for the total number of words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# word indices for each token\n",
    "print(tokenizer.word_index['in'])\n",
    "print(tokenizer.word_index['the'])\n",
    "print(tokenizer.word_index['town'])\n",
    "print(tokenizer.word_index['of'])\n",
    "print(tokenizer.word_index['athy'])\n",
    "print(tokenizer.word_index['one'])\n",
    "print(tokenizer.word_index['jeremy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in': 1, 'the': 2, 'town': 3, 'of': 4, 'athy': 5, 'one': 6, 'jeremy': 7, 'lanigan': 8, 'battered': 9, 'away': 10, 'till': 11, 'he': 12, 'hadnt': 13, 'had': 14, 'a': 15, 'pond': 16, 'his': 17, 'father': 18, 'died': 19, 'and': 20, 'made': 21, 'him': 22, 'rich': 23}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-b3d6f14b778b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-b3d6f14b778b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    model.add(Bidirectional(LSTM(units = 20)))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# EMB_DIM = total_words\n",
    "num_epochs = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length = (max_sequence_len - 1))\n",
    "model.add(Bidirectional(LSTM(units = 20)))\n",
    "model.add(Dense(total_words, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(xs, ys, epochs = num_epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
